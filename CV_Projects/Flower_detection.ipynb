{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flower Detect\n",
    "Identify and classify flower types based on their images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir=r\"D:\\Data centr\\IMG_data\\flowers\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Collection\n",
    "* Augment Data- Rotation, Croping, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch size is the number of training sample used in one forward and backward pass through the model during training.\n",
    "\n",
    "Sample= 1,000, batch_size = 32,\n",
    "At each iteration,process 32 samples.\n",
    "\n",
    "Iteration= 1000 ÷ 32 = ~32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_hgt=224\n",
    "img_wdt=224\n",
    "batch_sze=32 # 32 Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess the Data\n",
    "Build a CNN\n",
    "* Resize all images to a fixed size (224x224)\n",
    "* Normalize pixel values (0–1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen=ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,  # Preprocessing for MobileNetV2\n",
    "    rotation_range=30, # rotate image by 30 degree\n",
    "    width_shift_range=0.2,  # shift horizontally by 20% of width\n",
    "    height_shift_range=0.2,  # shift vertically by 20% of height\n",
    "    shear_range=0.2, # Shearing skews the image along one axis, making it look like the image is \"slanted.\"\n",
    "    zoom_range=0.2, # zooms in or out by up to 20%\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    validation_split=0.2 # Split for training and validation set by 20% of images\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the dataset\n",
    "Train_data & Validation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 604 images belonging to 11 classes.\n"
     ]
    }
   ],
   "source": [
    "train_data = train_datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=(img_hgt, img_wdt),\n",
    "    batch_size=batch_sze,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 145 images belonging to 11 classes.\n"
     ]
    }
   ],
   "source": [
    "val_data=train_datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=(img_hgt,img_wdt),\n",
    "    batch_size=batch_sze,\n",
    "    class_mode=\"categorical\",\n",
    "    subset='validation'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model\n",
    "* Loss Function: Use categorical_crossentropy for multi-class classification.\n",
    "* Optimizer: Use Adam with a learning rate of 0.001.\n",
    "* Epochs: The number of times the entire dataset is processed during training.\n",
    "Start with 20–50 epochs, depending on the dataset size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model=MobileNetV2( # pre trained deep learning model\n",
    "    weights='imagenet', # trained on the ImageNet dataset (1,000 categories like dogs, cats, cars, etc)\n",
    "    include_top=False, # Removes the final classification layer of MobileNetV2 and add custom layers for flower dataset.\n",
    "    input_shape=(img_hgt,img_wdt,3) # 3- color channer(RGB)\n",
    "    )\n",
    "base_model.trainable=False  # Freezing the base model means the weights of MobileNetV2 will not be updated during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = GlobalAveragePooling2D()(base_model.output)\n",
    "x = Dense(128, activation='relu')(x)  # Fully connected layer\n",
    "output_tensor = Dense(5, activation='softmax')(x)  # Output layer for 5 classes\n",
    "x = Dense(128, activation='relu')(x)\n",
    "output_tensor = Dense(5, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential([\n",
    "    base_model, #frozen MobileNetV2 model, which acts as a feature extractor (e.g., shapes, textures, etc).\n",
    "    tf.keras.layers.GlobalAveragePooling2D(), # Converts a 2D feature map into a 1D feature vector.\n",
    "    Dense(128,activation='relu'), # Fully connected layer with 128 neurons and ReLU activation.\n",
    "    Dropout(0.3), # Prevents overfitting by randomly setting 30% of the neurons to 0 during training\n",
    "    Dense(train_data.num_classes,activation='softmax') # Output layer for flower types\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adam is an adaptive optimizer that adjusts learning rates during training for better convergence.\n",
    "\n",
    "Categorical_crossentropy: Measures the difference between the predicted probability distribution (output of softmax) and the true labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adam(learning_rate=0.001),loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\himan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 822ms/step - accuracy: 0.2551 - loss: 2.3559"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\himan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 1s/step - accuracy: 0.2638 - loss: 2.3301 - val_accuracy: 0.7103 - val_loss: 0.9261\n",
      "Epoch 2/10\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 1s/step - accuracy: 0.7489 - loss: 0.8097 - val_accuracy: 0.8690 - val_loss: 0.5530\n",
      "Epoch 3/10\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 1s/step - accuracy: 0.8285 - loss: 0.5349 - val_accuracy: 0.8966 - val_loss: 0.3799\n",
      "Epoch 4/10\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.8605 - loss: 0.4128 - val_accuracy: 0.8828 - val_loss: 0.3904\n",
      "Epoch 5/10\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.9071 - loss: 0.3126 - val_accuracy: 0.8552 - val_loss: 0.4625\n",
      "Epoch 6/10\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.9026 - loss: 0.3242 - val_accuracy: 0.8897 - val_loss: 0.3704\n",
      "Epoch 7/10\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - accuracy: 0.9308 - loss: 0.2307 - val_accuracy: 0.9379 - val_loss: 0.2993\n",
      "Epoch 8/10\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 1s/step - accuracy: 0.9418 - loss: 0.2092 - val_accuracy: 0.9172 - val_loss: 0.3011\n",
      "Epoch 9/10\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1s/step - accuracy: 0.9260 - loss: 0.2734 - val_accuracy: 0.8966 - val_loss: 0.3899\n",
      "Epoch 10/10\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.9447 - loss: 0.2015 - val_accuracy: 0.8828 - val_loss: 0.3578\n"
     ]
    }
   ],
   "source": [
    "epochs=10\n",
    "history=model.fit(train_data,validation_data=val_data,epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the Model\n",
    "* Use accuracy, precision, recall, and F1-score to measure performance.\n",
    "* Plot training and validation loss/accuracy to ensure no overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 732ms/step - accuracy: 0.9160 - loss: 0.3132\n",
      "validation Accuracy:91.72%\n"
     ]
    }
   ],
   "source": [
    "val_loss,val_accuracy=model.evaluate(val_data)\n",
    "print(f\"validation Accuracy:{val_accuracy*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ mobilenetv2_1.00_224            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                    │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_1      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">163,968</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,419</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ mobilenetv2_1.00_224            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     │     \u001b[38;5;34m2,257,984\u001b[0m │\n",
       "│ (\u001b[38;5;33mFunctional\u001b[0m)                    │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d_1      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m163,968\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m)             │         \u001b[38;5;34m1,419\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,754,147</span> (10.51 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,754,147\u001b[0m (10.51 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">165,387</span> (646.04 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m165,387\u001b[0m (646.04 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">330,776</span> (1.26 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m330,776\u001b[0m (1.26 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "model.save('flower_classifier_model.h5')\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Label: ['HRaj', 'bougainvillea', 'daisies', 'garden_rose', 'gardenias', 'hibiscus', 'hydrangeas', 'lilies', 'orchids', 'peonies', 'tulip']\n"
     ]
    }
   ],
   "source": [
    "class_name=list(train_data.class_indices.keys()) # A dictionary mapping class names to integer labels list.\n",
    "print(\"Class Label:\",class_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_flower(image_path):\n",
    "    # Load and preprocess the image\n",
    "    img=image.load_img(image_path,target_size=(img_hgt,img_wdt))\n",
    "    img_array=image.img_to_array(img)\n",
    "    img_array=np.expand_dims(img_array,axis=0)\n",
    "    img_array=preprocess_input(img_array)\n",
    "\n",
    "    # Predict the flower type\n",
    "    prediction=model.predict(img_array)\n",
    "    prediction_class=class_name[np.argmax(prediction)]\n",
    "    confidence=np.max(prediction)\n",
    "\n",
    "\n",
    "    \"\"\"    \n",
    "    # Display the image with prediction\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.imshow(plt.imread(image_path))  # Load the image for display\n",
    "    plt.axis('off')  # Remove axis labels\n",
    "    plt.title(f\"Predicted: {prediction_class}\\nConfidence: {confidence * 100:.2f}%\")\n",
    "    plt.show()\n",
    "    \"\"\"\n",
    "    \n",
    "    return prediction_class,confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Rescaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\himan\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\preprocessing\\tf_data_layer.py:19: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "model_preprocessed=tf.keras.Sequential([\n",
    "    Rescaling(1./255, input_shape=(224,224,3)), # Normalize layer\n",
    "    model\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\himan\\AppData\\Local\\Temp\\tmpe3rgli5i\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\himan\\AppData\\Local\\Temp\\tmpe3rgli5i\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\himan\\AppData\\Local\\Temp\\tmpe3rgli5i'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='keras_tensor_165')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 11), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  2478362558928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478362561808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478362561232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478362561424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478362562192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478362562384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478362559888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478362561616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478362560272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478362562768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478362563728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478362563344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478362560656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478362564496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478362564112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478362566608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478362567376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478362566992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478362568336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478362567952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478362569104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478362570064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478362569872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478362568720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478362567568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478362571408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478362572752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478362572944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478362572560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478362571792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478362573328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478362574672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478362574480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478362574288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478362574096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478363001296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478363002640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478363002832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478363002448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478363001872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478363001104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478363004752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478363004944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478363004560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478363002064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478363003216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478363006864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478363007056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478363006672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478363004176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478363007440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478363008784: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478363008976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478363008592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478363007824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478363009360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478363010704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478363010896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478363010512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478363009744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478363011280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478363012624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478363012816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478363012432: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478363011664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478363013200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478363014544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478363014736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478363014352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478363013584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478363015120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478363016464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478363016656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478363016272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478363015504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478363015888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478363444048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478363443280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478363444240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478363443472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478363445008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478363446352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478363446544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478363446160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478363445392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478363446928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478363448272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478363448464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478363448080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478363447312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478363448848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478363450192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478363450384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478363450000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478363449232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478363450768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478363452112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478363452304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478363451920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478363451152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478363452688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478363454032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478363454224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478363453840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478363453072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478363454608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478363455952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478363456144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478363455760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478363454992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478363456528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478363457872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478363458064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478363457680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478363456912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478363458448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478363458832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478380400720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478363444624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478363459216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478380400912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478380402832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478380403024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478380402640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478380401872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478380403408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478380404752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478380404944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478380404560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478380403792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478380405328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478380406672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478380406864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478380406480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478380405712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478380407248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478380408592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478380408784: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478380408400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478380407632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478380409168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478380410512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478380410704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478380410320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478380409552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478380411088: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478380412432: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478380412624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478380412240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478380411472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478380413008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478380414352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478380414544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478380414160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478380413392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478380414928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478380416272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478380416464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478380416080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478380415312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478380415696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478380827472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478380826704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478380827664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478380826896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478380828432: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478380829776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478380829968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478380829584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478380828816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478380830352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478380831696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478380831888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478380831504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478380830736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478380832272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478380833616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478380833808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478380833424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478380832656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478380834192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478380835536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478380835728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478380835344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478380834576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478380836112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478380837456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478380837648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478380837264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478380836496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478380838032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478380839376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478380839568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478380839184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478380838416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478380839952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478380841296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478380841488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478380841104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478380840336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478380841872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478380842256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478381220496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478380828048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478380842640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478381220688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478381222032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478381222224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478381221840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478381221072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478381222608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478381223952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478381224144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478381223760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478381222992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478381224528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478381225872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478381226064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478381225680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478381224912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478381226448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478381227792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478381227984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478381227600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478381226832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478381228368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478381229712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478381229904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478381229520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478381228752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478381230288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478381231632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478381231824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478381231440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478381230672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478381232208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478381233552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478381233744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478381233360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478381232592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478381234128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478381235472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478381235664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478381235280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478381234512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478381234896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478404649808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478404649040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478404650000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478404649232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478404650768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478404652112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478404652304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478404651920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478404651152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478404652688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478404654032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478404654224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478404653840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478404653072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478404657872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478404659600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478404659792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2478404660368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "Help on class TFLiteConverterV2 in module tensorflow.lite.python.lite:\n",
      "\n",
      "class TFLiteConverterV2(TFLiteFrozenGraphConverterV2)\n",
      " |  TFLiteConverterV2(funcs, trackable_obj=None)\n",
      " |\n",
      " |  Converts a TensorFlow model into TensorFlow Lite model.\n",
      " |\n",
      " |  Attributes:\n",
      " |    optimizations: Experimental flag, subject to change. Set of optimizations to\n",
      " |      apply. e.g {tf.lite.Optimize.DEFAULT}. (default None, must be None or a\n",
      " |      set of values of type `tf.lite.Optimize`)\n",
      " |    representative_dataset: A generator function used for integer quantization\n",
      " |      where each generated sample has the same order, type and shape as the\n",
      " |      inputs to the model. Usually, this is a small subset of a few hundred\n",
      " |      samples randomly chosen, in no particular order, from the training or\n",
      " |      evaluation dataset. This is an optional attribute, but required for full\n",
      " |      integer quantization, i.e, if `tf.int8` is the only supported type in\n",
      " |      `target_spec.supported_types`. Refer to `tf.lite.RepresentativeDataset`.\n",
      " |      (default None)\n",
      " |    target_spec: Experimental flag, subject to change. Specifications of the\n",
      " |      target device, including supported ops set, supported types and a set\n",
      " |      of user's defined TensorFlow operators required in the TensorFlow Lite\n",
      " |      runtime. Refer to `tf.lite.TargetSpec`.\n",
      " |    inference_input_type: Data type of the input layer. Note that integer types\n",
      " |      (tf.int8 and tf.uint8) are currently only supported for post-training\n",
      " |      integer quantization and quantization-aware training. (default tf.float32,\n",
      " |      must be in {tf.float32, tf.int8, tf.uint8})\n",
      " |    inference_output_type: Data type of the output layer. Note that integer\n",
      " |      types (tf.int8 and tf.uint8) are currently only supported for\n",
      " |      post-training integer quantization and quantization-aware training.\n",
      " |      (default tf.float32, must be in {tf.float32, tf.int8, tf.uint8})\n",
      " |    allow_custom_ops: Boolean indicating whether to allow custom operations.\n",
      " |      When False, any unknown operation is an error. When True, custom ops are\n",
      " |      created for any op that is unknown. The developer needs to provide these\n",
      " |      to the TensorFlow Lite runtime with a custom resolver. (default False)\n",
      " |    exclude_conversion_metadata: Whether not to embed the conversion metadata\n",
      " |      into the converted model. (default False)\n",
      " |    experimental_new_converter: Experimental flag, subject to change. Enables\n",
      " |      MLIR-based conversion. (default True)\n",
      " |    experimental_new_quantizer: Experimental flag, subject to change. Enables\n",
      " |      MLIR-based quantization conversion instead of Flatbuffer-based conversion.\n",
      " |      (default True)\n",
      " |    experimental_enable_resource_variables: Experimental flag, subject to\n",
      " |      change. Enables [resource\n",
      " |      variables](https://tensorflow.org/guide/migrate/tf1_vs_tf2#resourcevariables_instead_of_referencevariables)\n",
      " |      to be converted by this converter. This is only allowed if the\n",
      " |      from_saved_model interface is used. (default True)\n",
      " |\n",
      " |  Example usage:\n",
      " |\n",
      " |  ```python\n",
      " |  # Converting a SavedModel to a TensorFlow Lite model.\n",
      " |  converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
      " |  tflite_model = converter.convert()\n",
      " |\n",
      " |  # Converting a tf.Keras model to a TensorFlow Lite model.\n",
      " |  converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
      " |  tflite_model = converter.convert()\n",
      " |\n",
      " |  # Converting ConcreteFunctions to a TensorFlow Lite model.\n",
      " |  converter = tf.lite.TFLiteConverter.from_concrete_functions([func], model)\n",
      " |  tflite_model = converter.convert()\n",
      " |\n",
      " |  # Converting a Jax model to a TensorFlow Lite model.\n",
      " |  converter = tf.lite.TFLiteConverter.experimental_from_jax(\n",
      " |      [func], [[ ('input1', input1), ('input2', input2)]])\n",
      " |  tflite_model = converter.convert()\n",
      " |  ```\n",
      " |\n",
      " |  Method resolution order:\n",
      " |      TFLiteConverterV2\n",
      " |      TFLiteFrozenGraphConverterV2\n",
      " |      TFLiteConverterBaseV2\n",
      " |      TFLiteConverterBase\n",
      " |      builtins.object\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  __init__(self, funcs, trackable_obj=None)\n",
      " |      Constructor for TFLiteConverter.\n",
      " |\n",
      " |      Args:\n",
      " |        funcs: List of TensorFlow ConcreteFunctions. The list should not contain\n",
      " |          duplicate elements.\n",
      " |        trackable_obj: tf.AutoTrackable object associated with `funcs`. A\n",
      " |          reference to this object needs to be maintained so that Variables do not\n",
      " |          get garbage collected since functions have a weak reference to\n",
      " |          Variables. This is only required when the tf.AutoTrackable object is not\n",
      " |          maintained by the user (e.g. `from_saved_model`).\n",
      " |\n",
      " |  convert(self)\n",
      " |      Converts a TensorFlow GraphDef based on instance variables.\n",
      " |\n",
      " |      Returns:\n",
      " |        The converted data in serialized format.\n",
      " |\n",
      " |      Raises:\n",
      " |        ValueError:\n",
      " |          No concrete function is specified.\n",
      " |          Multiple concrete functions are specified.\n",
      " |          Input shape is not specified.\n",
      " |          Invalid quantization parameters.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |\n",
      " |  experimental_from_jax(serving_funcs, inputs)\n",
      " |      Creates a TFLiteConverter object from a Jax model with its inputs. (deprecated)\n",
      " |\n",
      " |      Deprecated: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      Use `jax2tf.convert` and (`lite.TFLiteConverter.from_saved_model` or `lite.TFLiteConverter.from_concrete_functions`) instead.\n",
      " |\n",
      " |      Args:\n",
      " |        serving_funcs: An array of Jax functions with all the weights applied\n",
      " |          already.\n",
      " |        inputs: An array of Jax input placeholders tuples list, e.g.,\n",
      " |          jnp.zeros(INPUT_SHAPE). Each tuple list should correspond with the\n",
      " |          serving function.\n",
      " |\n",
      " |      Returns:\n",
      " |        TFLiteConverter object.\n",
      " |\n",
      " |  from_concrete_functions(funcs, trackable_obj=None)\n",
      " |      Creates a TFLiteConverter object from ConcreteFunctions.\n",
      " |\n",
      " |      Args:\n",
      " |        funcs: List of TensorFlow ConcreteFunctions. The list should not contain\n",
      " |          duplicate elements. Currently converter can only convert a single\n",
      " |          ConcreteFunction. Converting multiple functions is under development.\n",
      " |        trackable_obj:   An `AutoTrackable` object (typically `tf.module`)\n",
      " |          associated with `funcs`. A reference to this object needs to be\n",
      " |          maintained so that Variables do not get garbage collected since\n",
      " |          functions have a weak reference to Variables.\n",
      " |\n",
      " |      Returns:\n",
      " |        TFLiteConverter object.\n",
      " |\n",
      " |      Raises:\n",
      " |        Invalid input type.\n",
      " |\n",
      " |  from_keras_model(model)\n",
      " |      Creates a TFLiteConverter object from a Keras model.\n",
      " |\n",
      " |      Args:\n",
      " |        model: tf.Keras.Model\n",
      " |\n",
      " |      Returns:\n",
      " |        TFLiteConverter object.\n",
      " |\n",
      " |  from_saved_model(saved_model_dir, signature_keys=None, tags=None)\n",
      " |      Creates a TFLiteConverter object from a SavedModel directory.\n",
      " |\n",
      " |      Args:\n",
      " |        saved_model_dir: SavedModel directory to convert.\n",
      " |        signature_keys: List of keys identifying SignatureDef containing inputs\n",
      " |          and outputs. Elements should not be duplicated. By default the\n",
      " |          `signatures` attribute of the MetaGraphdef is used. (default\n",
      " |          saved_model.signatures)\n",
      " |        tags: Set of tags identifying the MetaGraphDef within the SavedModel to\n",
      " |          analyze. All tags in the tag set must be present. (default\n",
      " |          {tf.saved_model.SERVING} or {'serve'})\n",
      " |\n",
      " |      Returns:\n",
      " |        TFLiteConverter object.\n",
      " |\n",
      " |      Raises:\n",
      " |        Invalid signature keys.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from TFLiteConverterBase:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "converter=tf.lite.TFLiteConverter.from_keras_model(model_preprocessed)\n",
    "tflite_model=converter.convert()\n",
    "\n",
    "with open('flower_classifier_model.tflite','wb') as f:\n",
    "    f.write(tflite_model)\n",
    "print(help(tf.lite.TFLiteConverter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tflite_support'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtflite_support\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetadata_writers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m image_classifier\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtflite_support\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetadata_writers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m writer_utils\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tflite_support'"
     ]
    }
   ],
   "source": [
    "from tflite_support.metadata_writers import image_classifier\n",
    "from tflite_support.metadata_writers import writer_utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tflite_support'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtflite_support\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetadata_writers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m image_classifier\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtflite_support\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetadata_writers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m writer_utils\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Paths\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tflite_support'"
     ]
    }
   ],
   "source": [
    "from tflite_support.metadata_writers import image_classifier\n",
    "from tflite_support.metadata_writers import writer_utils\n",
    "\n",
    "# Paths\n",
    "model_path = \"flower_classifier_model.tflite\"\n",
    "export_path = \"flower_classifier_with_metadata.tflite\"\n",
    "\n",
    "# Image metadata\n",
    "input_norm_mean = 127.5\n",
    "input_norm_std = 127.5\n",
    "label_file_path = \"labels.txt\"  # Ensure this file contains class names, one per line\n",
    "\n",
    "# Create metadata writer\n",
    "writer = image_classifier.MetadataWriter.create_for_inference(\n",
    "    writer_utils.load_file(model_path),\n",
    "    input_norm_mean=input_norm_mean,\n",
    "    input_norm_std=input_norm_std,\n",
    "    label_file=label_file_path\n",
    ")\n",
    "\n",
    "# Write metadata to file\n",
    "writer_utils.save_file(writer.populate(), export_path)\n",
    "print(f\"Metadata added to {export_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_pth=r\"D:\\Data centr\\IMG_data\\flowers\\hydrangeas\\hydrangeas_00010.jpg\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "Predicted Flower Type: hydrangeas, Confidence: 99.55%\n"
     ]
    }
   ],
   "source": [
    "flower_type,confidence=predict_flower(img_pth)\n",
    "print(f\"Predicted Flower Type: {flower_type}, Confidence: {confidence * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
